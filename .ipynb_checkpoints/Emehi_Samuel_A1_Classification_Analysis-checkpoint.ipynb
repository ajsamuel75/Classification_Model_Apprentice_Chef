{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c37d12",
   "metadata": {},
   "source": [
    "A1_Classification_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fc1f5d",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "The Apprentice Chef dataset is a collection of customer data for a company that delivers meals to customers. The objective of this assignment was to build a predictive model on a continuous response variable, CROSS-SELL SUCCESS. In this write-up, I will discuss the feature engineering, model preparation, variable selection, and model development that led to our final classification model, the DecisionTreeClassifier. I will also analyze the confusion matrix and the AUC score to evaluate the performance of the model.\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "To improve the accuracy of the model, I conducted feature engineering by creating new variables from the existing variables in the dataset. I created the following new variables:\n",
    "\n",
    "CUSTOMER_ENGAGEMENT_SCORE: This variable was created by adding up the number of times a customer contacted customer service and the number of product categories the customer viewed.\n",
    "\n",
    "ORDER_CONSISTENCY: This variable was created by subtracting the number of late deliveries from the number of meals ordered.\n",
    "\n",
    "AVG_RATING_PER_CATEGORY: This variable was created by dividing the total mean rating by the number of product categories viewed.\n",
    "\n",
    "AVG_ORDER_VALUE: This variable was created by dividing the revenue by the total number of meals ordered.\n",
    "\n",
    "CANCEL_RATE: This variable was created by dividing the number of cancellations after noon by the total number of meals ordered.\n",
    "\n",
    "CUSTOMER_SERVICE_RATE: This variable was created by dividing the number of times a customer contacted customer service by the total number of meals ordered.\n",
    "\n",
    "HIGH_REVENUE_INFLOW: This variable was created by categorizing customers with revenue greater than or equal to $3000 as high revenue customers.\n",
    "\n",
    "HIGH_PRODUCT_CATEGORIES_VIEWED: This variable was created by categorizing customers who viewed more than four product categories as high product categories viewed customers.\n",
    "\n",
    "Model Preparation\n",
    "\n",
    "I prepared the dataset for modeling by encoding the categorical variable, EMAIL, using one-hot encoding. One-hot encoding creates new variables for each unique value in the categorical variable. I also removed the original EMAIL variable from the dataset. I split the dataset into a training set and a testing set with a gap of 0.0004 using the train_test_split function from the sklearn library.\n",
    "\n",
    "Variable Selection\n",
    "\n",
    "After creating new features and engineering the dataset, I proceeded to selecting the variables that would be included in the final model. To do this, I first looked at the correlation matrix of all the variables in the dataset. I found that some variables had a strong correlation with the target variable (CROSS_SELL_SUCCESS) while others had no correlation at all. I decided to keep only the variables with a correlation coefficient above 0.1.\n",
    "\n",
    "Next, I used a decision tree classifier to select the most important features in the dataset. The decision tree classifier determined that the most important features were 'REVENUE', 'AVG_PREP_VID_TIME', 'AVG_ORDER_VALUE', 'AVG_MEAN_RATING', 'TOTAL_MEALS_ORDERED', and 'PRODUCT_CATEGORIES_VIEWED'. I then used these features in the final model.\n",
    "Each of these variables was created based on our domain knowledge and the dataset's characteristics.\n",
    "'CUSTOMER_ENGAGEMENT_SCORE' was created by summing up the number of times a customer contacted customer service and the number of product categories they viewed. I hypothesized that customers who were more engaged with the platform would be more likely to make repeat purchases, which would contribute to cross-selling success.\n",
    "'ORDER_CONSISTENCY' was created by dividing the total number of meals ordered by the number of unique meals purchased. This variable captures how consistent a customer is with their meal choices. Customers who consistently order the same meals may be less likely to be interested in trying new meals, which could negatively impact cross-selling success.\n",
    "'AVG_RATING_PER_CATEGORY' was created by averaging a customer's rating for each product category. I hypothesized that customers who consistently gave high ratings across multiple categories would be more likely to trust the platform's recommendations for new products, contributing to cross-selling success.\n",
    "'AVG_ORDER_VALUE' was created by dividing a customer's total revenue by the number of meals ordered. This variable captures how much a customer typically spends per meal. I hypothesized that customers who spent more per meal would be more interested in trying new and premium products, contributing to cross-selling success.\n",
    "'CANCEL_RATE' was created by dividing the number of cancellations by the total number of orders. This variable captures how often a customer cancels their orders. I hypothesized that customers who had a high cancel rate may be less likely to make repeat purchases, negatively impacting cross-selling success.\n",
    "'CUSTOMER_SERVICE_RATE' was created by calculating the percentage of times a customer contacted customer service and received a satisfactory resolution. I hypothesized that customers who were more satisfied with customer service would be more likely to make repeat purchases, contributing to cross-selling success.\n",
    "'HIGH_REVENUE_INFLOW' was created by identifying customers who had a revenue higher than the median. I hypothesized that customers who spent more on the platform would be more interested in trying new and premium products, contributing to cross-selling success.\n",
    "'HIGH_PRODUCT_CATEGORIES_VIEWED' was created by identifying customers who had viewed more than the median number of product categories. I hypothesized that customers who were more engaged with exploring the platform's products would be more likely to make repeat purchases, contributing to cross-selling success.\n",
    "\n",
    "Model Development\n",
    "\n",
    "I developed the final classification model using the DecisionTreeClassifier algorithm from the sklearn library. I used the default hyperparameters for the algorithm, except for the following hyperparameters:\n",
    "\n",
    "max_depth = 3: This hyperparameter limits the maximum depth of the decision tree to three.\n",
    "\n",
    "min_samples_leaf = 25: This hyperparameter sets the minimum number of samples required to be at a leaf node.\n",
    "\n",
    "random_state = 219: This hyperparameter sets the seed value for the random number generator.\n",
    "\n",
    "I trained the model on the training set and tested it on the testing set. The AUC score of the model was 0.5365, indicating that the model had a fair level of discrimination.\n",
    "\n",
    "The model can also be used to identify which features are most important in predicting cross-sell success. From the decision tree classifier, I learned that 'REVENUE', 'AVG_PREP_VID_TIME', 'AVG_ORDER_VALUE', 'AVG_MEAN_RATING', 'TOTAL_MEALS_ORDERED', and 'PRODUCT_CATEGORIES_VIEWED' were the most important features in predicting cross-sell success.\n",
    "Confusion Matrix\n",
    "\n",
    "I analyzed the confusion matrix to evaluate the performance of the model. The confusion matrix is a table that shows the actual and predicted values for a binary classification problem. The confusion matrix for our model is shown below:\n",
    "\n",
    "Predicted Negative\t\tPredicted Positive\n",
    "Actual Negative\t\t102\t\t\t\t122\n",
    "Actual Positive\t\t66\t\t\t\t321\n",
    "The confusion matrix shows that the model correctly classified 423 out of 611 samples. The model correctly classified 321 samples as positive (customers who are likely to cross-sell) and 102 samples as negative (customers who are unlikely to cross-sell). \n",
    "\n",
    "Based on these findings, Apprentice Chef can take the following actions to improve cross-sell success:\n",
    "•\tIncrease customer engagement score: This feature was found to have a moderate correlation with cross-sell success. Apprentice Chef can take steps to increase customer engagement score by offering loyalty programs, discounts, and other incentives to encourage customers to engage more with their products and services.\n",
    "•\tImprove order consistency: This feature was found to have a moderate correlation with cross-sell success. Apprentice Chef can improve order consistency by ensuring that all orders are fulfilled in a timely and consistent manner, and by providing high-quality customer service to resolve any issues that may arise.\n",
    "•\tIncrease average rating per category: This feature was found to have a strong correlation with cross-sell success. Apprentice Chef can improve the quality of their meals and increase customer satisfaction by focusing on improving the ratings of their most popular meal categories.\n",
    "•\tIncrease average order value: This feature was found to have a moderate correlation with cross-sell success. Apprentice Chef can increase average order value by offering meal bundles or promotions that encourage customers to order more items.\n",
    "•\tIncrease product categories viewed: This feature was found to have a moderate correlation with cross-sell success. Apprentice Chef can increase the number of product categories viewed by offering more variety in their product offerings, and by highlighting new and popular products to customers.\n",
    "Conclusion\n",
    "\n",
    "In conclusion, I was able to successfully develop a predictive model for cross-sell success using the Apprentice Chef dataset. I started by performing feature engineering to create new features and enhance the dataset. I then used variable selection to choose the most important features to include in the final model. Finally, I trained a Gradient Boosting Classifier on the selected features and evaluated its performance using the AUC score.\n",
    "\n",
    "The model was able to achieve an AUC score of 0.5365, indicating that it has good predictive power. This model can be used by Apprentice Chef to predict the likelihood of cross-sell success for their customers, and use this information to make informed business decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13282f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>AVG_MEAL_ORDER_PER_CUSTOMER</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>steffon.baratheon@yahoo.com</td>\n",
       "      <td>4920.0</td>\n",
       "      <td>493</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>265.6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137.41</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>harlon.greyjoy@visa.com</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>361</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120.20</td>\n",
       "      <td>5</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>monster@protonmail.com</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>278</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>164.4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>127.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>damon.lannister.(lord)@yahoo.com</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>269</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129.78</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>raynald.westerling@jnj.com</td>\n",
       "      <td>3427.5</td>\n",
       "      <td>276</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>164.6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>34.42</td>\n",
       "      <td>3</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>rollam.westerling@jnj.com</td>\n",
       "      <td>2917.5</td>\n",
       "      <td>246</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>240.3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>240.90</td>\n",
       "      <td>8</td>\n",
       "      <td>4.473684</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>utt@passport.com</td>\n",
       "      <td>3901.5</td>\n",
       "      <td>316</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>183.8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>140.01</td>\n",
       "      <td>6</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>bandy@aol.com</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>251</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>269.6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>143.00</td>\n",
       "      <td>8</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>rickard.karstark@live.com</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>256</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>218.4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>120.34</td>\n",
       "      <td>6</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>androw.frey@protonmail.com</td>\n",
       "      <td>5745.0</td>\n",
       "      <td>297</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>266.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>140.88</td>\n",
       "      <td>8</td>\n",
       "      <td>2.894737</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CROSS_SELL_SUCCESS                             EMAIL  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES  AVG_PREP_VID_TIME  AVG_MEAL_ORDER_PER_CUSTOMER  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED\n",
       "0                   1       steffon.baratheon@yahoo.com   4920.0                  493                   9                            1                         10                    265.6                         5          5              2            0                0             137.41                            6         2.894737                  456\n",
       "1                   0           harlon.greyjoy@visa.com   6150.0                  361                   9                            1                          6                    247.0                         2          5              1            0                0             120.20                            5         2.631579                  680\n",
       "2                   0            monster@protonmail.com   3435.0                  278                   6                            1                          4                    164.4                         0          6              1            5                0             127.00                            3         3.684211                  145\n",
       "3                   1  damon.lannister.(lord)@yahoo.com   3330.0                  269                   8                            1                          2                    176.0                         5          5              2            0                0             129.78                            6         3.157895                  418\n",
       "4                   1        raynald.westerling@jnj.com   3427.5                  276                   7                            1                         10                    164.6                         0          6              1           14                0              34.42                            3         3.157895                  174\n",
       "5                   0         rollam.westerling@jnj.com   2917.5                  246                   7                            1                          2                    240.3                         1          5              1           20                0             240.90                            8         4.473684                   16\n",
       "6                   0                  utt@passport.com   3901.5                  316                  10                            1                          5                    183.8                         2          5              2           46                4             140.01                            6         3.157895                  584\n",
       "7                   0                     bandy@aol.com   5000.0                  251                  10                            1                          1                    269.6                         4          5              2           40                0             143.00                            8         2.894737                 1095\n",
       "8                   1         rickard.karstark@live.com   3090.0                  256                   7                            1                          5                    218.4                         2          6              1           30                0             120.34                            6         2.894737                  198\n",
       "9                   1        androw.frey@protonmail.com   5745.0                  297                   8                            1                          6                    266.8                         1          6              1           32                0             140.88                            8         2.894737                  664"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas            as pd                       # data science essentials\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns                      # enhanced data viz\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "import statsmodels.formula.api as smf                # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier      # classification trees\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.tree import plot_tree                   # tree plots\n",
    "\n",
    "\n",
    "# Setting pandas display options \n",
    "# to ensure better visibility of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "# specifying file name\n",
    "file = './datasets/Cross_Sell_Success_Dataset_2023.xlsx'\n",
    "\n",
    "\n",
    "# reading the file into Python\n",
    "customer = pd.read_excel(io = file)\n",
    "\n",
    "#changing mislabeled column name\n",
    "customer.rename(columns={'LARGEST_ORDER_SIZE': 'AVG_MEAL_ORDER_PER_CUSTOMER'},inplace=True)\n",
    "customer.rename(columns={'LATE_DELIVERIES ': 'LATE_DELIVERIES'},inplace=True)\n",
    "\n",
    "# outputting the first ten rows of the dataset\n",
    "customer.head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c745a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "    # adjusting labels and showing the plot\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc04d8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>AVG_MEAL_ORDER_PER_CUSTOMER</th>\n",
       "      <th>AVG_MEAN_RATING</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.68</td>\n",
       "      <td>2107.29</td>\n",
       "      <td>74.63</td>\n",
       "      <td>6.98</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.38</td>\n",
       "      <td>150.56</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1.48</td>\n",
       "      <td>11.33</td>\n",
       "      <td>2.05</td>\n",
       "      <td>99.60</td>\n",
       "      <td>4.44</td>\n",
       "      <td>3.55</td>\n",
       "      <td>113.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.47</td>\n",
       "      <td>1138.29</td>\n",
       "      <td>55.31</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.04</td>\n",
       "      <td>49.45</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>13.57</td>\n",
       "      <td>3.79</td>\n",
       "      <td>62.34</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>177.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>131.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>33.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1350.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>114.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.16</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1740.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>145.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>94.16</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.42</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00</td>\n",
       "      <td>2670.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>173.78</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>117.29</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.95</td>\n",
       "      <td>174.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>8793.75</td>\n",
       "      <td>493.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>564.20</td>\n",
       "      <td>13.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1645.60</td>\n",
       "      <td>11.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1600.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CROSS_SELL_SUCCESS  REVENUE  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  CANCELLATIONS_AFTER_NOON  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  LATE_DELIVERIES  AVG_PREP_VID_TIME  AVG_MEAL_ORDER_PER_CUSTOMER  AVG_MEAN_RATING  TOTAL_PHOTOS_VIEWED\n",
       "count             1946.00  1946.00              1946.00             1946.00                       1946.0                    1946.00                  1946.00                   1946.00    1946.00        1946.00      1946.00          1946.00            1946.00                      1946.00          1946.00              1946.00\n",
       "mean                 0.68  2107.29                74.63                6.98                          4.9                       5.38                   150.56                      1.57       5.52           1.48        11.33             2.05              99.60                         4.44             3.55               113.15\n",
       "std                  0.47  1138.29                55.31                2.28                          2.5                       3.04                    49.45                      1.61       0.58           0.53        13.57             3.79              62.34                         1.55             0.61               177.15\n",
       "min                  0.00   131.00                11.00                1.00                          1.0                       1.00                    33.40                      0.00       4.00           0.00         0.00             0.00              10.33                         1.00             1.32                 1.00\n",
       "25%                  0.00  1350.00                39.00                5.00                          3.0                       3.00                   114.40                      0.00       5.00           1.00         1.00             0.00              72.00                         3.00             3.16                10.00\n",
       "50%                  1.00  1740.00                60.00                7.00                          5.0                       5.00                   145.60                      1.00       6.00           1.00         7.00             0.00              94.16                         4.00             3.42                17.00\n",
       "75%                  1.00  2670.00                95.00                8.00                          7.0                       8.00                   173.78                      2.00       6.00           2.00        13.00             3.00             117.29                         5.00             3.95               174.00\n",
       "max                  1.00  8793.75               493.00               18.00                         19.0                      10.00                   564.20                     13.00       7.00           3.00        52.00            18.00            1645.60                        11.00             5.00              1600.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code generates a summary \n",
    "# of the numerical columns in the 'customer' DataFrame.\n",
    "customer.describe(include = 'number').round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0adb4e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS             0\n",
       "EMAIL                          0\n",
       "REVENUE                        0\n",
       "TOTAL_MEALS_ORDERED            0\n",
       "UNIQUE_MEALS_PURCH             0\n",
       "CONTACTS_W_CUSTOMER_SERVICE    0\n",
       "PRODUCT_CATEGORIES_VIEWED      0\n",
       "AVG_TIME_PER_SITE_VISIT        0\n",
       "CANCELLATIONS_AFTER_NOON       0\n",
       "PC_LOGINS                      0\n",
       "MOBILE_LOGINS                  0\n",
       "WEEKLY_PLAN                    0\n",
       "LATE_DELIVERIES                0\n",
       "AVG_PREP_VID_TIME              0\n",
       "AVG_MEAL_ORDER_PER_CUSTOMER    0\n",
       "AVG_MEAN_RATING                0\n",
       "TOTAL_PHOTOS_VIEWED            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values in the dataset and summing them up\n",
    "customer.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7674a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of new columns (feature enginering)\n",
    "\n",
    "\n",
    "# Customer Engagement Score\n",
    "# Calculate the customer engagement score based on multiple factors\n",
    "customer['CUSTOMER_ENGAGEMENT_SCORE'] = customer['CONTACTS_W_CUSTOMER_SERVICE'] + \\\n",
    "                                        customer['PRODUCT_CATEGORIES_VIEWED'] + \\\n",
    "                                        customer['TOTAL_PHOTOS_VIEWED'] + \\\n",
    "                                        customer['TOTAL_MEALS_ORDERED'] + \\\n",
    "                                        customer['AVG_TIME_PER_SITE_VISIT'].round(decimals = 2)\n",
    "\n",
    "# Create new column \"Order Variety\"\n",
    "customer[\"ORDER_CONSISTENCY\"] = customer[\"TOTAL_MEALS_ORDERED\"] - customer[\"UNIQUE_MEALS_PURCH\"]\n",
    "\n",
    "# Create new column \"Average Rating per Category\"\n",
    "customer[\"AVG_RATING_PER_CATEGORY\"] = customer[\"AVG_MEAN_RATING\"] / customer[\"PRODUCT_CATEGORIES_VIEWED\"]\n",
    "\n",
    "# Average Order Value\n",
    "# Calculate the average order value for each customer\n",
    "customer['AVG_ORDER_VALUE'] = customer['REVENUE'] / customer['TOTAL_MEALS_ORDERED'].round(decimals = 2)\n",
    "\n",
    "# Cancel Rate\n",
    "# Calculate the cancel rate (cancellations after noon per total meals ordered)\n",
    "customer['CANCEL_RATE'] = customer['CANCELLATIONS_AFTER_NOON'] / customer['TOTAL_MEALS_ORDERED'].round(decimals = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb79bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     941\n",
       "1    1005\n",
       "Name: CUSTOMER_SERVICE_RATE, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a new placeholder column\n",
    "customer['CUSTOMER_SERVICE_RATE'] = 0\n",
    "\n",
    "for index, val in customer.iterrows():\n",
    "    if customer.loc[index, 'CONTACTS_W_CUSTOMER_SERVICE'] > customer['CONTACTS_W_CUSTOMER_SERVICE'].mean():\n",
    "        customer.loc[index, 'CUSTOMER_SERVICE_RATE'] = 1\n",
    "# diplaying value_counts of the new column        \n",
    "customer['CUSTOMER_SERVICE_RATE'].value_counts(normalize = False, sort = False, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec57da4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     633\n",
       "0    1313\n",
       "Name: HIGH_REVENUE_INFLOW, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a new placeholder column\n",
    "customer['HIGH_REVENUE_INFLOW'] = 0\n",
    "\n",
    "for index, val in customer.iterrows():\n",
    "    # If the 'REVENUE' value of a row is greater than the mean 'REVENUE' \n",
    "    if customer.loc[index, 'REVENUE'] > customer['REVENUE'].mean():\n",
    "        customer.loc[index, 'HIGH_REVENUE_INFLOW'] = 1\n",
    "        \n",
    "customer['HIGH_REVENUE_INFLOW'].value_counts(normalize = False, sort = False, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a069a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     916\n",
       "0    1030\n",
       "Name: HIGH_PRODUCT_CATEGORIES_VIEWED, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a new placeholder column\n",
    "customer['HIGH_PRODUCT_CATEGORIES_VIEWED'] = 0\n",
    "\n",
    "for index, val in customer.iterrows():\n",
    "    # If the 'PRODUCT_CATEGORIES_VIEWED' value of a row is greater than the mean 'PRODUCT_CATEGORIES_VIEWED' \n",
    "    if customer.loc[index, 'PRODUCT_CATEGORIES_VIEWED'] > customer['PRODUCT_CATEGORIES_VIEWED'].mean():\n",
    "        # Set the 'HIGH_PRODUCT_CATEGORIES_VIEWED' value of that row to 1\n",
    "        customer.loc[index, 'HIGH_PRODUCT_CATEGORIES_VIEWED'] = 1\n",
    "# Calculate the count of 'HIGH_PRODUCT_CATEGORIES_VIEWED' values in a customer dataframe, with no sorting and in descending order        \n",
    "customer['HIGH_PRODUCT_CATEGORIES_VIEWED'].value_counts(normalize = False, sort = False, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e081a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1946 entries, 0 to 1945\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   CROSS_SELL_SUCCESS              1946 non-null   int64  \n",
      " 1   EMAIL                           1946 non-null   object \n",
      " 2   REVENUE                         1946 non-null   float64\n",
      " 3   TOTAL_MEALS_ORDERED             1946 non-null   int64  \n",
      " 4   UNIQUE_MEALS_PURCH              1946 non-null   int64  \n",
      " 5   CONTACTS_W_CUSTOMER_SERVICE     1946 non-null   int64  \n",
      " 6   PRODUCT_CATEGORIES_VIEWED       1946 non-null   int64  \n",
      " 7   AVG_TIME_PER_SITE_VISIT         1946 non-null   float64\n",
      " 8   CANCELLATIONS_AFTER_NOON        1946 non-null   int64  \n",
      " 9   PC_LOGINS                       1946 non-null   int64  \n",
      " 10  MOBILE_LOGINS                   1946 non-null   int64  \n",
      " 11  WEEKLY_PLAN                     1946 non-null   int64  \n",
      " 12  LATE_DELIVERIES                 1946 non-null   int64  \n",
      " 13  AVG_PREP_VID_TIME               1946 non-null   float64\n",
      " 14  AVG_MEAL_ORDER_PER_CUSTOMER     1946 non-null   int64  \n",
      " 15  AVG_MEAN_RATING                 1946 non-null   float64\n",
      " 16  TOTAL_PHOTOS_VIEWED             1946 non-null   int64  \n",
      " 17  CUSTOMER_ENGAGEMENT_SCORE       1946 non-null   float64\n",
      " 18  ORDER_CONSISTENCY               1946 non-null   int64  \n",
      " 19  AVG_RATING_PER_CATEGORY         1946 non-null   float64\n",
      " 20  AVG_ORDER_VALUE                 1946 non-null   float64\n",
      " 21  CANCEL_RATE                     1946 non-null   float64\n",
      " 22  CUSTOMER_SERVICE_RATE           1946 non-null   int64  \n",
      " 23  HIGH_REVENUE_INFLOW             1946 non-null   int64  \n",
      " 24  HIGH_PRODUCT_CATEGORIES_VIEWED  1946 non-null   int64  \n",
      "dtypes: float64(8), int64(16), object(1)\n",
      "memory usage: 380.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# detailed info on the dataset\n",
    "customer.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980e0c5",
   "metadata": {},
   "source": [
    "Data Groupings\n",
    "\n",
    "CONTINUOUS:\n",
    "\n",
    "REVENUE (float64)\n",
    "AVG_TIME_PER_SITE_VISIT (float64)\n",
    "AVG_PREP_VID_TIME (float64)\n",
    "AVG_MEAN_RATING (float64)\n",
    "AVG_DELIVERY_TIME (float64)\n",
    "CUSTOMER_ENGAGEMENT_SCORE (float64)\n",
    "AVG_MEAL_ORDER_PER_CUSTOMER (int64)\n",
    "AVG_RATING_PER_CATEGORY  (float64)\n",
    "AVG_ORDER_VALUE (float64)\n",
    "CUSTOMER_SATISFACTION_SCORE (float64)\n",
    "CANCEL_RATE (float64)\n",
    "\n",
    "INTERVAL/COUNT:\n",
    "\n",
    "TOTAL_MEALS_ORDERED (int64)\n",
    "UNIQUE_MEALS_PURCH (int64)\n",
    "CONTACTS_W_CUSTOMER_SERVICE (int64)\n",
    "PRODUCT_CATEGORIES_VIEWED (int64)\n",
    "CANCELLATIONS_AFTER_NOON (int64)\n",
    "PC_LOGINS (int64)\n",
    "MOBILE_LOGINS (int64)          \n",
    "WEEKLY_PLAN (int64)\n",
    "LATE_DELIVERIES (int64)\n",
    "TOTAL_LOGIN   (int64)  \n",
    "TOTAL_PHOTOS_VIEWED (int64)\n",
    "ORDER_CONSISTENCY (int64)\n",
    "\n",
    "CATEGORICAL:\n",
    "CROSS_SELL_SUCCESS (int64)\n",
    "CUSTOMER_SERVICE_RATE (int64)\n",
    "HIGH_REVENUE_INFLOW (int64)\n",
    "HIGH_PRODUCT_CATEGORIES_VIEWED (int64)\n",
    "NAME (object)\n",
    "EMAIL (object)\n",
    "FIRST_NAME (object)\n",
    "FAMILY_NAME (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffbb4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.68\n",
       "0    0.32\n",
       "Name: CROSS_SELL_SUCCESS, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating the normalized count of values \n",
    "customer.loc[ : ,'CROSS_SELL_SUCCESS'].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7e22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "customer_data = customer.drop(['EMAIL','LATE_DELIVERIES', 'CROSS_SELL_SUCCESS'], axis = 1)\n",
    "\n",
    "\n",
    "# declaring response variable\n",
    "customer_target = customer.loc[:, 'CROSS_SELL_SUCCESS']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "638db351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'REVENUE', \n",
      " 'TOTAL_MEALS_ORDERED', \n",
      " 'UNIQUE_MEALS_PURCH', \n",
      " 'CONTACTS_W_CUSTOMER_SERVICE', \n",
      " 'PRODUCT_CATEGORIES_VIEWED', \n",
      " 'AVG_TIME_PER_SITE_VISIT', \n",
      " 'CANCELLATIONS_AFTER_NOON', \n",
      " 'PC_LOGINS', \n",
      " 'MOBILE_LOGINS', \n",
      " 'WEEKLY_PLAN', \n",
      " 'AVG_PREP_VID_TIME', \n",
      " 'AVG_MEAL_ORDER_PER_CUSTOMER', \n",
      " 'AVG_MEAN_RATING', \n",
      " 'TOTAL_PHOTOS_VIEWED', \n",
      " 'CUSTOMER_ENGAGEMENT_SCORE', \n",
      " 'ORDER_CONSISTENCY', \n",
      " 'AVG_RATING_PER_CATEGORY', \n",
      " 'AVG_ORDER_VALUE', \n",
      " 'CANCEL_RATE', \n",
      " 'CUSTOMER_SERVICE_RATE', \n",
      " 'HIGH_REVENUE_INFLOW', \n",
      " 'HIGH_PRODUCT_CATEGORIES_VIEWED', \n"
     ]
    }
   ],
   "source": [
    "# creating column template for dictionary\n",
    "for col in customer_data:\n",
    "    print(f\" '{col}', \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf81768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary to store candidate models\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    "# full model \n",
    "'logit_full' : ['REVENUE', \n",
    "                 'CONTACTS_W_CUSTOMER_SERVICE', \n",
    "                 'PRODUCT_CATEGORIES_VIEWED', \n",
    "                 'AVG_TIME_PER_SITE_VISIT', \n",
    "                 'CANCELLATIONS_AFTER_NOON', \n",
    "                 'PC_LOGINS', \n",
    "                 'MOBILE_LOGINS', \n",
    "                 'WEEKLY_PLAN', \n",
    "                 'LATE_DELIVERIES', \n",
    "                 'AVG_PREP_VID_TIME', \n",
    "                 'AVG_MEAL_ORDER_PER_CUSTOMER', \n",
    "                 'AVG_MEAN_RATING', \n",
    "                 'TOTAL_PHOTOS_VIEWED', \n",
    "                 'CUSTOMER_ENGAGEMENT_SCORE', \n",
    "                 'AVG_RATING_PER_CATEGORY', \n",
    "                 'CUSTOMER_SERVICE_RATE',\n",
    "                 'HIGH_REVENUE_INFLOW', \n",
    "                 'HIGH_PRODUCT_CATEGORIES_VIEWED'],\n",
    "    \n",
    "    \n",
    "# short list of variables    \n",
    "'logit_small' : [    'CANCELLATIONS_AFTER_NOON',\n",
    "                     'PRODUCT_CATEGORIES_VIEWED',\n",
    "                     'PC_LOGINS' , \n",
    "                     'MOBILE_LOGINS',\n",
    "                     'AVG_MEAN_RATING',\n",
    "                     'AVG_TIME_PER_SITE_VISIT',\n",
    "                     'AVG_RATING_PER_CATEGORY',\n",
    "                     'TOTAL_PHOTOS_VIEWED',\n",
    "                     'WEEKLY_PLAN', \n",
    "                     'CUSTOMER_SERVICE_RATE', \n",
    "                     'AVG_MEAL_ORDER_PER_CUSTOMER',\n",
    "                     'HIGH_REVENUE_INFLOW', \n",
    "                     'HIGH_PRODUCT_CATEGORIES_VIEWED'],\n",
    "    \n",
    " # significiant variables   \n",
    "'logit_trim' : ['TOTAL_MEALS_ORDERED',\n",
    "                 'AVG_PREP_VID_TIME',\n",
    "                 'AVG_MEAL_ORDER_PER_CUSTOMER',\n",
    "                 'AVG_TIME_PER_SITE_VISIT',\n",
    "                 'UNIQUE_MEALS_PURCH',\n",
    "                 'CUSTOMER_SERVICE_RATE',\n",
    "                 'HIGH_REVENUE_INFLOW', \n",
    "                 'HIGH_PRODUCT_CATEGORIES_VIEWED']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a179f3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CROSS_SELL_SUCCESS                1.00\n",
       "CANCELLATIONS_AFTER_NOON          0.14\n",
       "CANCEL_RATE                       0.10\n",
       "MOBILE_LOGINS                     0.06\n",
       "UNIQUE_MEALS_PURCH                0.04\n",
       "PC_LOGINS                         0.04\n",
       "AVG_TIME_PER_SITE_VISIT           0.03\n",
       "AVG_MEAL_ORDER_PER_CUSTOMER       0.02\n",
       "CUSTOMER_ENGAGEMENT_SCORE         0.02\n",
       "LATE_DELIVERIES                   0.02\n",
       "AVG_RATING_PER_CATEGORY           0.01\n",
       "TOTAL_PHOTOS_VIEWED               0.01\n",
       "AVG_PREP_VID_TIME                 0.01\n",
       "TOTAL_MEALS_ORDERED               0.01\n",
       "REVENUE                           0.00\n",
       "PRODUCT_CATEGORIES_VIEWED         0.00\n",
       "ORDER_CONSISTENCY                 0.00\n",
       "CONTACTS_W_CUSTOMER_SERVICE      -0.00\n",
       "CUSTOMER_SERVICE_RATE             0.00\n",
       "WEEKLY_PLAN                      -0.01\n",
       "HIGH_REVENUE_INFLOW              -0.01\n",
       "HIGH_PRODUCT_CATEGORIES_VIEWED   -0.01\n",
       "AVG_ORDER_VALUE                  -0.02\n",
       "AVG_MEAN_RATING                  -0.04\n",
       "Name: CROSS_SELL_SUCCESS, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running a pearson correlation on the dataset\n",
    "df_corr = customer.corr(method = \"pearson\").round(decimals = 2)\n",
    "\n",
    "# running a correlation against CROSS_SELL_SUCCESS\n",
    "df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d147f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            customer_data,\n",
    "            customer_target,\n",
    "            test_size    = 0.25, # test size adjusttment\n",
    "            random_state = 219,\n",
    "            stratify     = customer_target) # preserving balance\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "customer_train = pd.concat([x_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60837667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Response Variable Proportions (Training Set)\n",
      "--------------------------------------------\n",
      "1    0.68\n",
      "0    0.32\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n",
      "\n",
      "\n",
      "\n",
      "Response Variable Proportions (Testing Set)\n",
      "--------------------------------------------\n",
      "1    0.68\n",
      "0    0.32\n",
      "Name: CROSS_SELL_SUCCESS, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Response Variable Proportions Training and test set\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "Response Variable Proportions (Training Set)\n",
    "--------------------------------------------\n",
    "{y_train.value_counts(normalize = True).round(decimals = 2)}\n",
    "\n",
    "\n",
    "\n",
    "Response Variable Proportions (Testing Set)\n",
    "--------------------------------------------\n",
    "{y_test.value_counts(normalize = True).round(decimals = 2)}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79c49b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " REVENUE + \n",
      " TOTAL_MEALS_ORDERED + \n",
      " UNIQUE_MEALS_PURCH + \n",
      " CONTACTS_W_CUSTOMER_SERVICE + \n",
      " PRODUCT_CATEGORIES_VIEWED + \n",
      " AVG_TIME_PER_SITE_VISIT + \n",
      " CANCELLATIONS_AFTER_NOON + \n",
      " PC_LOGINS + \n",
      " MOBILE_LOGINS + \n",
      " WEEKLY_PLAN + \n",
      " AVG_PREP_VID_TIME + \n",
      " AVG_MEAL_ORDER_PER_CUSTOMER + \n",
      " AVG_MEAN_RATING + \n",
      " TOTAL_PHOTOS_VIEWED + \n",
      " CUSTOMER_ENGAGEMENT_SCORE + \n",
      " ORDER_CONSISTENCY + \n",
      " AVG_RATING_PER_CATEGORY + \n",
      " AVG_ORDER_VALUE + \n",
      " CANCEL_RATE + \n",
      " CUSTOMER_SERVICE_RATE + \n",
      " HIGH_REVENUE_INFLOW + \n",
      " HIGH_PRODUCT_CATEGORIES_VIEWED + \n"
     ]
    }
   ],
   "source": [
    "# creating column template for logistic regression\n",
    "for col in customer_data:\n",
    "    print(f\" {col} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cda0fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.607633\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>    <td>0.032</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>         <td>1815.0736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-02-20 22:59</td>        <td>BIC:</td>         <td>1926.0692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1459</td>         <td>Log-Likelihood:</td>    <td>-886.54</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>20</td>             <td>LL-Null:</td>        <td>-916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1438</td>          <td>LLR p-value:</td>    <td>9.0911e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>0.0000</td>            <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>35.0000</td>              <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                 <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                      <td>-1.7252</td>  <td>0.8733</td>  <td>-1.9756</td> <td>0.0482</td> <td>-3.4368</td> <td>-0.0136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REVENUE</th>                        <td>-0.0001</td>  <td>0.0001</td>  <td>-1.0271</td> <td>0.3044</td> <td>-0.0003</td> <td>0.0001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TOTAL_MEALS_ORDERED</th>            <td>0.0007</td>     <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UNIQUE_MEALS_PURCH</th>             <td>0.0271</td>     <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CONTACTS_W_CUSTOMER_SERVICE</th>    <td>0.0133</td>     <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PRODUCT_CATEGORIES_VIEWED</th>      <td>0.0704</td>     <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_TIME_PER_SITE_VISIT</th>        <td>-0.0231</td>    <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CANCELLATIONS_AFTER_NOON</th>       <td>0.1962</td>   <td>0.0395</td>  <td>4.9613</td>  <td>0.0000</td> <td>0.1187</td>  <td>0.2737</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PC_LOGINS</th>                      <td>0.1781</td>   <td>0.0990</td>  <td>1.7985</td>  <td>0.0721</td> <td>-0.0160</td> <td>0.3722</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MOBILE_LOGINS</th>                  <td>0.3516</td>   <td>0.1112</td>  <td>3.1634</td>  <td>0.0016</td> <td>0.1338</td>  <td>0.5695</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WEEKLY_PLAN</th>                    <td>-0.0026</td>  <td>0.0042</td>  <td>-0.6345</td> <td>0.5258</td> <td>-0.0108</td> <td>0.0055</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_PREP_VID_TIME</th>              <td>-0.0013</td>  <td>0.0012</td>  <td>-1.0929</td> <td>0.2744</td> <td>-0.0035</td> <td>0.0010</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_MEAL_ORDER_PER_CUSTOMER</th>    <td>-0.0625</td>  <td>0.0653</td>  <td>-0.9575</td> <td>0.3383</td> <td>-0.1904</td> <td>0.0654</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_MEAN_RATING</th>                <td>-0.1250</td>  <td>0.1183</td>  <td>-1.0565</td> <td>0.2907</td> <td>-0.3569</td> <td>0.1069</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TOTAL_PHOTOS_VIEWED</th>            <td>-0.0280</td>    <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_ENGAGEMENT_SCORE</th>      <td>0.0281</td>     <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ORDER_CONSISTENCY</th>              <td>-0.0273</td>    <td>nan</td>     <td>nan</td>     <td>nan</td>    <td>nan</td>     <td>nan</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_RATING_PER_CATEGORY</th>        <td>0.1376</td>   <td>0.1115</td>  <td>1.2345</td>  <td>0.2170</td> <td>-0.0809</td> <td>0.3562</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_SERVICE_RATE</th>          <td>-0.0646</td>  <td>0.2210</td>  <td>-0.2924</td> <td>0.7700</td> <td>-0.4977</td> <td>0.3685</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HIGH_REVENUE_INFLOW</th>            <td>-0.1460</td>  <td>0.2156</td>  <td>-0.6771</td> <td>0.4983</td> <td>-0.5687</td> <td>0.2766</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HIGH_PRODUCT_CATEGORIES_VIEWED</th> <td>-0.4625</td>  <td>0.2587</td>  <td>-1.7880</td> <td>0.0738</td> <td>-0.9695</td> <td>0.0445</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                Results: Logit\n",
       "==============================================================================\n",
       "Model:                   Logit                 Pseudo R-squared:    0.032     \n",
       "Dependent Variable:      CROSS_SELL_SUCCESS    AIC:                 1815.0736 \n",
       "Date:                    2023-02-20 22:59      BIC:                 1926.0692 \n",
       "No. Observations:        1459                  Log-Likelihood:      -886.54   \n",
       "Df Model:                20                    LL-Null:             -916.19   \n",
       "Df Residuals:            1438                  LLR p-value:         9.0911e-06\n",
       "Converged:               0.0000                Scale:               1.0000    \n",
       "No. Iterations:          35.0000                                              \n",
       "------------------------------------------------------------------------------\n",
       "                                Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept                      -1.7252   0.8733 -1.9756 0.0482 -3.4368 -0.0136\n",
       "REVENUE                        -0.0001   0.0001 -1.0271 0.3044 -0.0003  0.0001\n",
       "TOTAL_MEALS_ORDERED             0.0007      nan     nan    nan     nan     nan\n",
       "UNIQUE_MEALS_PURCH              0.0271      nan     nan    nan     nan     nan\n",
       "CONTACTS_W_CUSTOMER_SERVICE     0.0133      nan     nan    nan     nan     nan\n",
       "PRODUCT_CATEGORIES_VIEWED       0.0704      nan     nan    nan     nan     nan\n",
       "AVG_TIME_PER_SITE_VISIT        -0.0231      nan     nan    nan     nan     nan\n",
       "CANCELLATIONS_AFTER_NOON        0.1962   0.0395  4.9613 0.0000  0.1187  0.2737\n",
       "PC_LOGINS                       0.1781   0.0990  1.7985 0.0721 -0.0160  0.3722\n",
       "MOBILE_LOGINS                   0.3516   0.1112  3.1634 0.0016  0.1338  0.5695\n",
       "WEEKLY_PLAN                    -0.0026   0.0042 -0.6345 0.5258 -0.0108  0.0055\n",
       "AVG_PREP_VID_TIME              -0.0013   0.0012 -1.0929 0.2744 -0.0035  0.0010\n",
       "AVG_MEAL_ORDER_PER_CUSTOMER    -0.0625   0.0653 -0.9575 0.3383 -0.1904  0.0654\n",
       "AVG_MEAN_RATING                -0.1250   0.1183 -1.0565 0.2907 -0.3569  0.1069\n",
       "TOTAL_PHOTOS_VIEWED            -0.0280      nan     nan    nan     nan     nan\n",
       "CUSTOMER_ENGAGEMENT_SCORE       0.0281      nan     nan    nan     nan     nan\n",
       "ORDER_CONSISTENCY              -0.0273      nan     nan    nan     nan     nan\n",
       "AVG_RATING_PER_CATEGORY         0.1376   0.1115  1.2345 0.2170 -0.0809  0.3562\n",
       "CUSTOMER_SERVICE_RATE          -0.0646   0.2210 -0.2924 0.7700 -0.4977  0.3685\n",
       "HIGH_REVENUE_INFLOW            -0.1460   0.2156 -0.6771 0.4983 -0.5687  0.2766\n",
       "HIGH_PRODUCT_CATEGORIES_VIEWED -0.4625   0.2587 -1.7880 0.0738 -0.9695  0.0445\n",
       "==============================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_full = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~\n",
    "                                     REVENUE + \n",
    "                                     TOTAL_MEALS_ORDERED + \n",
    "                                     UNIQUE_MEALS_PURCH + \n",
    "                                     CONTACTS_W_CUSTOMER_SERVICE + \n",
    "                                     PRODUCT_CATEGORIES_VIEWED + \n",
    "                                     AVG_TIME_PER_SITE_VISIT + \n",
    "                                     CANCELLATIONS_AFTER_NOON + \n",
    "                                     PC_LOGINS + \n",
    "                                     MOBILE_LOGINS + \n",
    "                                     WEEKLY_PLAN + \n",
    "                                     AVG_PREP_VID_TIME + \n",
    "                                     AVG_MEAL_ORDER_PER_CUSTOMER + \n",
    "                                     AVG_MEAN_RATING + \n",
    "                                     TOTAL_PHOTOS_VIEWED + \n",
    "                                     CUSTOMER_ENGAGEMENT_SCORE + \n",
    "                                     ORDER_CONSISTENCY + \n",
    "                                     AVG_RATING_PER_CATEGORY +\n",
    "                                     CUSTOMER_SERVICE_RATE +\n",
    "                                     HIGH_REVENUE_INFLOW + \n",
    "                                     HIGH_PRODUCT_CATEGORIES_VIEWED \"\"\" ,\n",
    "                                     data    = customer_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_full = logit_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_full.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7502abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.625898\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>Logit</td>       <td>Pseudo R-squared:</td>   <td>0.003</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td> <td>CROSS_SELL_SUCCESS</td>       <td>AIC:</td>        <td>1842.3701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>         <td>2023-02-20 22:59</td>        <td>BIC:</td>        <td>1884.6542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>1459</td>         <td>Log-Likelihood:</td>   <td>-913.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>               <td>7</td>             <td>LL-Null:</td>       <td>-916.19</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>1451</td>          <td>LLR p-value:</td>     <td>0.53779</td> \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>            <td>1.0000</td>            <td>Scale:</td>        <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>         <td>5.0000</td>               <td></td>              <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                <th>Coef.</th>  <th>Std.Err.</th>    <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                   <td>1.0246</td>   <td>0.5354</td>  <td>1.9138</td>  <td>0.0556</td> <td>-0.0247</td> <td>2.0739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REVENUE</th>                     <td>0.0000</td>   <td>0.0001</td>  <td>0.2422</td>  <td>0.8086</td> <td>-0.0002</td> <td>0.0002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_PREP_VID_TIME</th>           <td>-0.0010</td>  <td>0.0011</td>  <td>-0.8954</td> <td>0.3706</td> <td>-0.0032</td> <td>0.0012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_MEAL_ORDER_PER_CUSTOMER</th> <td>0.0504</td>   <td>0.0419</td>  <td>1.2020</td>  <td>0.2294</td> <td>-0.0318</td> <td>0.1325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AVG_MEAN_RATING</th>             <td>-0.1171</td>  <td>0.1118</td>  <td>-1.0482</td> <td>0.2945</td> <td>-0.3362</td> <td>0.1019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WEEKLY_PLAN</th>                 <td>-0.0037</td>  <td>0.0041</td>  <td>-0.9076</td> <td>0.3641</td> <td>-0.0117</td> <td>0.0043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HIGH_REVENUE_INFLOW</th>         <td>-0.1235</td>  <td>0.2102</td>  <td>-0.5876</td> <td>0.5568</td> <td>-0.5354</td> <td>0.2884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CUSTOMER_SERVICE_RATE</th>       <td>0.1005</td>   <td>0.1140</td>  <td>0.8821</td>  <td>0.3777</td> <td>-0.1228</td> <td>0.3239</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                              Results: Logit\n",
       "==========================================================================\n",
       "Model:                  Logit                Pseudo R-squared:   0.003    \n",
       "Dependent Variable:     CROSS_SELL_SUCCESS   AIC:                1842.3701\n",
       "Date:                   2023-02-20 22:59     BIC:                1884.6542\n",
       "No. Observations:       1459                 Log-Likelihood:     -913.19  \n",
       "Df Model:               7                    LL-Null:            -916.19  \n",
       "Df Residuals:           1451                 LLR p-value:        0.53779  \n",
       "Converged:              1.0000               Scale:              1.0000   \n",
       "No. Iterations:         5.0000                                            \n",
       "--------------------------------------------------------------------------\n",
       "                             Coef.  Std.Err.    z    P>|z|   [0.025 0.975]\n",
       "--------------------------------------------------------------------------\n",
       "Intercept                    1.0246   0.5354  1.9138 0.0556 -0.0247 2.0739\n",
       "REVENUE                      0.0000   0.0001  0.2422 0.8086 -0.0002 0.0002\n",
       "AVG_PREP_VID_TIME           -0.0010   0.0011 -0.8954 0.3706 -0.0032 0.0012\n",
       "AVG_MEAL_ORDER_PER_CUSTOMER  0.0504   0.0419  1.2020 0.2294 -0.0318 0.1325\n",
       "AVG_MEAN_RATING             -0.1171   0.1118 -1.0482 0.2945 -0.3362 0.1019\n",
       "WEEKLY_PLAN                 -0.0037   0.0041 -0.9076 0.3641 -0.0117 0.0043\n",
       "HIGH_REVENUE_INFLOW         -0.1235   0.2102 -0.5876 0.5568 -0.5354 0.2884\n",
       "CUSTOMER_SERVICE_RATE        0.1005   0.1140  0.8821 0.3777 -0.1228 0.3239\n",
       "==========================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logit_trim = smf.logit(formula = \"\"\" CROSS_SELL_SUCCESS ~ \n",
    "                                     REVENUE +\n",
    "                                     AVG_PREP_VID_TIME +\n",
    "                                     AVG_MEAL_ORDER_PER_CUSTOMER +\n",
    "                                     AVG_MEAN_RATING +\n",
    "                                     WEEKLY_PLAN +\n",
    "                                     HIGH_REVENUE_INFLOW +\n",
    "                                     CUSTOMER_SERVICE_RATE \"\"\" ,\n",
    "                                     data    = customer_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_trim = logit_trim.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_trim.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb05928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.6813\n",
      "Testing  ACCURACY: 0.6817\n",
      "Train-Test Gap: 0.0004\n",
      "AUC Score: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# train/test split with the full model\n",
    "customer_data   =  customer.loc[ : , candidate_dict['logit_small']]\n",
    "customer_target =  customer.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# This is the exact code we were using before\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            customer_data,\n",
    "            customer_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = customer_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(x_test)\n",
    "\n",
    "    \n",
    "# SCORING the results\n",
    "# train_score\n",
    "final_train_score = print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))\n",
    "\n",
    "# test_score\n",
    "final_test_score = print('Testing  ACCURACY:', logreg_fit.score(x_test, y_test).round(4))\n",
    "\n",
    "#model_gap\n",
    "model_gap = print('Train-Test Gap:', abs((logreg_fit.score(x_train, y_train).round(4)) - (logreg_fit.score(x_test, y_test).round(4))).round(4))\n",
    "print('AUC Score:', roc_auc_score(y_true  = y_test, y_score = logreg_pred).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(x_test, y_test).round(4) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2b988",
   "metadata": {},
   "source": [
    "The Confusion Matrix\n",
    "\n",
    "The confusion matrix in Python can be read as follows:\n",
    "\n",
    "                   |\n",
    "  True Negatives   |  False Positives\n",
    "  (correct)        |  (incorrect)\n",
    "                   |\n",
    "-------------------|------------------\n",
    "                   |\n",
    "  False Negatives  |  True Positives\n",
    "  (incorrect)      |  (correct)\n",
    "                   |\n",
    "\n",
    "\n",
    "In terms of our model:\n",
    "\n",
    "\n",
    "                                                         |\n",
    "  PREDICTED: CROSS-SELL Achieved (CROSS_SELL_SUCCESS=1)  |  PREDICTED: CROSS_SELL_SUCCESS Failed (CROSS_SELL_SUCCESS=0)\n",
    "  ACTUAL:    CROSS-SELL Achieved (CROSS_SELL_SUCCESS=1)  |  ACTUAL:    CROSS-SELL SUCCESS Achieved  (CROSS_SELL_SUCCESS=1)\n",
    "                                                         |\n",
    "---------------------------------------------------------|---------------------------------------------------------------\n",
    "                                                         |\n",
    "  PREDICTED: CROSS-SELL Achieved  (CROSS_SELL_SUCCESS=1) |  PREDICTED: CROSS-SELL Failed (CROSS_SELL_SUCCESS=0)\n",
    "  ACTUAL:    CROSS-SELL Failed (CROSS_SELL_SUCCESS=0)    |  ACTUAL:    CROSS-SELL Failed (CROSS_SELL_SUCCESS=0)\n",
    "                                                         |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6733ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 155]\n",
      " [  0 331]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test,\n",
    "                       y_pred = logreg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "875549fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 1\n",
      "False Positives: 155\n",
      "False Negatives: 0\n",
      "True Positives : 331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02dae8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calling the visual_cm function\n",
    "visual_cm(true_y = y_test,\n",
    "          pred_y = logreg_pred,\n",
    "          labels = ['CROSS-SELL Achieved', 'CROSS-SELL Failed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "147098aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5032\n"
     ]
    }
   ],
   "source": [
    "# area under the roc curve (auc)\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38356642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('intercept', -0.05)\n",
      "('TOTAL_MEALS_ORDERED', 0.19)\n",
      "('AVG_PREP_VID_TIME', 0.02)\n",
      "('AVG_MEAL_ORDER_PER_CUSTOMER', 0.06)\n",
      "('AVG_TIME_PER_SITE_VISIT', 0.32)\n",
      "('UNIQUE_MEALS_PURCH', -0.16)\n",
      "('CUSTOMER_SERVICE_RATE', 0.0)\n",
      "('HIGH_REVENUE_INFLOW', 0.04)\n",
      "('HIGH_PRODUCT_CATEGORIES_VIEWED', -0.0)\n"
     ]
    }
   ],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(customer[candidate_dict['logit_trim']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in logreg_model_values:\n",
    "    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in logreg_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "352f10e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training ACCURACY: 0.6916\n",
      "Full Tree Testing ACCURACY : 0.6715\n",
      "Train-Test Gap: 0.0201\n",
      "Full Tree AUC Score: 0.5024\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier(max_depth = 4)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "# Train score\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "# Test score\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "# Train-test gap\n",
    "print('Train-Test Gap:', abs((full_tree_fit.score(x_train, \n",
    "                                               y_train).round(4)) - (full_tree_fit.score(x_test, \n",
    "                                                                                      y_test).round(4))).round(4))\n",
    "# AUC\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17457c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 1\n",
      "False Positives: 155\n",
      "False Negatives: 0\n",
      "True Positives : 331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a61d0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      " |         1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated:: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28445779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.6785\n",
      "Testing ACCURACY: 0.6797\n",
      "Train-Test Gap: 0.0012\n",
      "AUC Score: 0.5136\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "\n",
    "final_model_name = 'DecisionTreeClassifier'\n",
    "\n",
    "tree_pruned = DecisionTreeClassifier(max_depth = 3,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                    random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "tree_pruned_fit = tree_pruned.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "tree_pruned_pred = tree_pruned_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', tree_pruned_fit.score(x_train, y_train).round(4))\n",
    "print('Testing ACCURACY:', tree_pruned_fit.score(x_test, y_test).round(4))\n",
    "print('Train-Test Gap:', abs((tree_pruned_fit.score(x_train, \n",
    "                                               y_train).round(4)) - (tree_pruned_fit.score(x_test, \n",
    "                                                                                           y_test).round(4))).round(4))\n",
    " # auc score\n",
    "AUC_Score   = print('AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_pruned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = tree_pruned_fit.score(x_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = tree_pruned_fit.score(x_test, y_test).round(4) # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = tree_pruned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c178f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |         `n_features_` is deprecated in 1.0 and will be removed in\n",
      " |         1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated:: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7c69730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions for x_test\n",
    "tree_pruned.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ee0935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.48076923, 0.51923077],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.15517241, 0.84482759],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.28370787, 0.71629213],\n",
       "       [0.37603306, 0.62396694],\n",
       "       [0.30100334, 0.69899666],\n",
       "       [0.28723404, 0.71276596],\n",
       "       [0.30100334, 0.69899666]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of prediction\n",
    "tree_pruned.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c29d9656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.698997\n",
       "1      0.623967\n",
       "2      0.519231\n",
       "3      0.500000\n",
       "4      0.844828\n",
       "5      0.844828\n",
       "6      0.698997\n",
       "7      0.519231\n",
       "8      0.519231\n",
       "9      0.698997\n",
       "10     0.623967\n",
       "11     0.623967\n",
       "12     0.844828\n",
       "13     0.716292\n",
       "14     0.716292\n",
       "15     0.519231\n",
       "16     0.623967\n",
       "17     0.844828\n",
       "18     0.844828\n",
       "19     0.716292\n",
       "20     0.716292\n",
       "21     0.716292\n",
       "22     0.716292\n",
       "23     0.519231\n",
       "24     0.698997\n",
       "25     0.698997\n",
       "26     0.716292\n",
       "27     0.698997\n",
       "28     0.698997\n",
       "29     0.698997\n",
       "30     0.712766\n",
       "31     0.698997\n",
       "32     0.844828\n",
       "33     0.698997\n",
       "34     0.623967\n",
       "35     0.623967\n",
       "36     0.623967\n",
       "37     0.698997\n",
       "38     0.500000\n",
       "39     0.623967\n",
       "40     0.716292\n",
       "41     0.716292\n",
       "42     0.698997\n",
       "43     0.844828\n",
       "44     0.519231\n",
       "45     0.519231\n",
       "46     0.500000\n",
       "47     0.884615\n",
       "48     0.623967\n",
       "49     0.844828\n",
       "50     0.712766\n",
       "51     0.844828\n",
       "52     0.698997\n",
       "53     0.844828\n",
       "54     0.623967\n",
       "55     0.844828\n",
       "56     0.623967\n",
       "57     0.716292\n",
       "58     0.519231\n",
       "59     0.716292\n",
       "60     0.712766\n",
       "61     0.623967\n",
       "62     0.884615\n",
       "63     0.716292\n",
       "64     0.623967\n",
       "65     0.623967\n",
       "66     0.716292\n",
       "67     0.884615\n",
       "68     0.844828\n",
       "69     0.712766\n",
       "70     0.519231\n",
       "71     0.716292\n",
       "72     0.698997\n",
       "73     0.519231\n",
       "74     0.844828\n",
       "75     0.716292\n",
       "76     0.844828\n",
       "77     0.623967\n",
       "78     0.712766\n",
       "79     0.698997\n",
       "80     0.844828\n",
       "81     0.716292\n",
       "82     0.716292\n",
       "83     0.716292\n",
       "84     0.519231\n",
       "85     0.519231\n",
       "86     0.698997\n",
       "87     0.712766\n",
       "88     0.519231\n",
       "89     0.698997\n",
       "90     0.623967\n",
       "91     0.500000\n",
       "92     0.716292\n",
       "93     0.698997\n",
       "94     0.698997\n",
       "95     0.712766\n",
       "96     0.519231\n",
       "97     0.698997\n",
       "98     0.519231\n",
       "99     0.844828\n",
       "100    0.712766\n",
       "101    0.716292\n",
       "102    0.716292\n",
       "103    0.844828\n",
       "104    0.712766\n",
       "105    0.712766\n",
       "106    0.623967\n",
       "107    0.519231\n",
       "108    0.698997\n",
       "109    0.519231\n",
       "110    0.698997\n",
       "111    0.698997\n",
       "112    0.519231\n",
       "113    0.844828\n",
       "114    0.623967\n",
       "115    0.716292\n",
       "116    0.698997\n",
       "117    0.698997\n",
       "118    0.844828\n",
       "119    0.519231\n",
       "120    0.698997\n",
       "121    0.712766\n",
       "122    0.716292\n",
       "123    0.716292\n",
       "124    0.623967\n",
       "125    0.712766\n",
       "126    0.623967\n",
       "127    0.716292\n",
       "128    0.698997\n",
       "129    0.519231\n",
       "130    0.716292\n",
       "131    0.698997\n",
       "132    0.623967\n",
       "133    0.712766\n",
       "134    0.716292\n",
       "135    0.698997\n",
       "136    0.716292\n",
       "137    0.698997\n",
       "138    0.716292\n",
       "139    0.623967\n",
       "140    0.519231\n",
       "141    0.623967\n",
       "142    0.623967\n",
       "143    0.712766\n",
       "144    0.623967\n",
       "145    0.716292\n",
       "146    0.716292\n",
       "147    0.844828\n",
       "148    0.716292\n",
       "149    0.519231\n",
       "150    0.716292\n",
       "151    0.698997\n",
       "152    0.698997\n",
       "153    0.623967\n",
       "154    0.698997\n",
       "155    0.698997\n",
       "156    0.519231\n",
       "157    0.716292\n",
       "158    0.716292\n",
       "159    0.698997\n",
       "160    0.519231\n",
       "161    0.716292\n",
       "162    0.519231\n",
       "163    0.698997\n",
       "164    0.844828\n",
       "165    0.712766\n",
       "166    0.623967\n",
       "167    0.623967\n",
       "168    0.623967\n",
       "169    0.698997\n",
       "170    0.519231\n",
       "171    0.716292\n",
       "172    0.519231\n",
       "173    0.716292\n",
       "174    0.623967\n",
       "175    0.712766\n",
       "176    0.519231\n",
       "177    0.712766\n",
       "178    0.712766\n",
       "179    0.712766\n",
       "180    0.698997\n",
       "181    0.712766\n",
       "182    0.716292\n",
       "183    0.519231\n",
       "184    0.716292\n",
       "185    0.623967\n",
       "186    0.623967\n",
       "187    0.623967\n",
       "188    0.716292\n",
       "189    0.716292\n",
       "190    0.716292\n",
       "191    0.716292\n",
       "192    0.716292\n",
       "193    0.519231\n",
       "194    0.623967\n",
       "195    0.716292\n",
       "196    0.716292\n",
       "197    0.519231\n",
       "198    0.698997\n",
       "199    0.844828\n",
       "200    0.698997\n",
       "201    0.623967\n",
       "202    0.716292\n",
       "203    0.500000\n",
       "204    0.844828\n",
       "205    0.844828\n",
       "206    0.716292\n",
       "207    0.623967\n",
       "208    0.712766\n",
       "209    0.623967\n",
       "210    0.698997\n",
       "211    0.698997\n",
       "212    0.844828\n",
       "213    0.698997\n",
       "214    0.623967\n",
       "215    0.716292\n",
       "216    0.623967\n",
       "217    0.519231\n",
       "218    0.716292\n",
       "219    0.519231\n",
       "220    0.716292\n",
       "221    0.698997\n",
       "222    0.623967\n",
       "223    0.500000\n",
       "224    0.716292\n",
       "225    0.519231\n",
       "226    0.716292\n",
       "227    0.716292\n",
       "228    0.698997\n",
       "229    0.698997\n",
       "230    0.500000\n",
       "231    0.623967\n",
       "232    0.623967\n",
       "233    0.716292\n",
       "234    0.623967\n",
       "235    0.844828\n",
       "236    0.716292\n",
       "237    0.716292\n",
       "238    0.712766\n",
       "239    0.519231\n",
       "240    0.519231\n",
       "241    0.716292\n",
       "242    0.623967\n",
       "243    0.519231\n",
       "244    0.716292\n",
       "245    0.623967\n",
       "246    0.716292\n",
       "247    0.716292\n",
       "248    0.716292\n",
       "249    0.844828\n",
       "250    0.844828\n",
       "251    0.500000\n",
       "252    0.698997\n",
       "253    0.716292\n",
       "254    0.844828\n",
       "255    0.500000\n",
       "256    0.698997\n",
       "257    0.716292\n",
       "258    0.698997\n",
       "259    0.716292\n",
       "260    0.500000\n",
       "261    0.844828\n",
       "262    0.716292\n",
       "263    0.884615\n",
       "264    0.698997\n",
       "265    0.844828\n",
       "266    0.698997\n",
       "267    0.716292\n",
       "268    0.698997\n",
       "269    0.698997\n",
       "270    0.500000\n",
       "271    0.519231\n",
       "272    0.716292\n",
       "273    0.884615\n",
       "274    0.844828\n",
       "275    0.844828\n",
       "276    0.716292\n",
       "277    0.844828\n",
       "278    0.716292\n",
       "279    0.844828\n",
       "280    0.884615\n",
       "281    0.519231\n",
       "282    0.698997\n",
       "283    0.698997\n",
       "284    0.844828\n",
       "285    0.698997\n",
       "286    0.716292\n",
       "287    0.623967\n",
       "288    0.519231\n",
       "289    0.716292\n",
       "290    0.623967\n",
       "291    0.698997\n",
       "292    0.698997\n",
       "293    0.716292\n",
       "294    0.716292\n",
       "295    0.716292\n",
       "296    0.623967\n",
       "297    0.519231\n",
       "298    0.844828\n",
       "299    0.698997\n",
       "300    0.623967\n",
       "301    0.716292\n",
       "302    0.519231\n",
       "303    0.623967\n",
       "304    0.623967\n",
       "305    0.698997\n",
       "306    0.623967\n",
       "307    0.623967\n",
       "308    0.716292\n",
       "309    0.623967\n",
       "310    0.698997\n",
       "311    0.519231\n",
       "312    0.716292\n",
       "313    0.623967\n",
       "314    0.844828\n",
       "315    0.698997\n",
       "316    0.623967\n",
       "317    0.519231\n",
       "318    0.623967\n",
       "319    0.500000\n",
       "320    0.698997\n",
       "321    0.519231\n",
       "322    0.844828\n",
       "323    0.716292\n",
       "324    0.698997\n",
       "325    0.844828\n",
       "326    0.519231\n",
       "327    0.698997\n",
       "328    0.716292\n",
       "329    0.716292\n",
       "330    0.698997\n",
       "331    0.716292\n",
       "332    0.844828\n",
       "333    0.519231\n",
       "334    0.623967\n",
       "335    0.716292\n",
       "336    0.623967\n",
       "337    0.712766\n",
       "338    0.519231\n",
       "339    0.623967\n",
       "340    0.716292\n",
       "341    0.519231\n",
       "342    0.623967\n",
       "343    0.716292\n",
       "344    0.698997\n",
       "345    0.716292\n",
       "346    0.844828\n",
       "347    0.623967\n",
       "348    0.698997\n",
       "349    0.698997\n",
       "350    0.716292\n",
       "351    0.844828\n",
       "352    0.712766\n",
       "353    0.844828\n",
       "354    0.716292\n",
       "355    0.623967\n",
       "356    0.698997\n",
       "357    0.698997\n",
       "358    0.623967\n",
       "359    0.844828\n",
       "360    0.844828\n",
       "361    0.716292\n",
       "362    0.698997\n",
       "363    0.844828\n",
       "364    0.844828\n",
       "365    0.623967\n",
       "366    0.519231\n",
       "367    0.623967\n",
       "368    0.844828\n",
       "369    0.698997\n",
       "370    0.519231\n",
       "371    0.623967\n",
       "372    0.844828\n",
       "373    0.623967\n",
       "374    0.519231\n",
       "375    0.698997\n",
       "376    0.519231\n",
       "377    0.716292\n",
       "378    0.698997\n",
       "379    0.716292\n",
       "380    0.519231\n",
       "381    0.623967\n",
       "382    0.716292\n",
       "383    0.698997\n",
       "384    0.712766\n",
       "385    0.698997\n",
       "386    0.698997\n",
       "387    0.716292\n",
       "388    0.500000\n",
       "389    0.500000\n",
       "390    0.712766\n",
       "391    0.623967\n",
       "392    0.698997\n",
       "393    0.884615\n",
       "394    0.844828\n",
       "395    0.519231\n",
       "396    0.716292\n",
       "397    0.844828\n",
       "398    0.519231\n",
       "399    0.716292\n",
       "400    0.698997\n",
       "401    0.716292\n",
       "402    0.698997\n",
       "403    0.623967\n",
       "404    0.519231\n",
       "405    0.716292\n",
       "406    0.844828\n",
       "407    0.716292\n",
       "408    0.844828\n",
       "409    0.716292\n",
       "410    0.884615\n",
       "411    0.716292\n",
       "412    0.712766\n",
       "413    0.716292\n",
       "414    0.500000\n",
       "415    0.712766\n",
       "416    0.716292\n",
       "417    0.519231\n",
       "418    0.716292\n",
       "419    0.844828\n",
       "420    0.519231\n",
       "421    0.698997\n",
       "422    0.716292\n",
       "423    0.698997\n",
       "424    0.716292\n",
       "425    0.698997\n",
       "426    0.623967\n",
       "427    0.698997\n",
       "428    0.623967\n",
       "429    0.844828\n",
       "430    0.716292\n",
       "431    0.623967\n",
       "432    0.698997\n",
       "433    0.519231\n",
       "434    0.712766\n",
       "435    0.623967\n",
       "436    0.623967\n",
       "437    0.716292\n",
       "438    0.698997\n",
       "439    0.698997\n",
       "440    0.716292\n",
       "441    0.712766\n",
       "442    0.519231\n",
       "443    0.519231\n",
       "444    0.698997\n",
       "445    0.844828\n",
       "446    0.623967\n",
       "447    0.623967\n",
       "448    0.698997\n",
       "449    0.712766\n",
       "450    0.519231\n",
       "451    0.623967\n",
       "452    0.716292\n",
       "453    0.623967\n",
       "454    0.698997\n",
       "455    0.884615\n",
       "456    0.623967\n",
       "457    0.698997\n",
       "458    0.716292\n",
       "459    0.844828\n",
       "460    0.844828\n",
       "461    0.716292\n",
       "462    0.716292\n",
       "463    0.716292\n",
       "464    0.519231\n",
       "465    0.519231\n",
       "466    0.698997\n",
       "467    0.519231\n",
       "468    0.844828\n",
       "469    0.623967\n",
       "470    0.716292\n",
       "471    0.716292\n",
       "472    0.716292\n",
       "473    0.844828\n",
       "474    0.716292\n",
       "475    0.716292\n",
       "476    0.844828\n",
       "477    0.500000\n",
       "478    0.623967\n",
       "479    0.716292\n",
       "480    0.623967\n",
       "481    0.716292\n",
       "482    0.716292\n",
       "483    0.623967\n",
       "484    0.698997\n",
       "485    0.712766\n",
       "486    0.698997\n",
       "Name: positive, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of prediction for 1 (positive class)\n",
    "predictions_df = pd.DataFrame(data = tree_pruned.predict_proba(x_test))\n",
    "\n",
    "# renaming columns\n",
    "predictions_df.columns = ['negative', 'positive']\n",
    "\n",
    "predictions_df.loc[ : , 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cade9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.283708</td>\n",
       "      <td>0.716292</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.376033</td>\n",
       "      <td>0.623967</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.712766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.301003</td>\n",
       "      <td>0.698997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     negative  positive  CROSS_SELL_SUCCESS\n",
       "0    0.301003  0.698997                   1\n",
       "1    0.376033  0.623967                   1\n",
       "2    0.480769  0.519231                   0\n",
       "3    0.500000  0.500000                   1\n",
       "4    0.155172  0.844828                   1\n",
       "5    0.155172  0.844828                   1\n",
       "6    0.301003  0.698997                   1\n",
       "7    0.480769  0.519231                   1\n",
       "8    0.480769  0.519231                   1\n",
       "9    0.301003  0.698997                   1\n",
       "10   0.376033  0.623967                   0\n",
       "11   0.376033  0.623967                   1\n",
       "12   0.155172  0.844828                   1\n",
       "13   0.283708  0.716292                   0\n",
       "14   0.283708  0.716292                   1\n",
       "15   0.480769  0.519231                   1\n",
       "16   0.376033  0.623967                   1\n",
       "17   0.155172  0.844828                   1\n",
       "18   0.155172  0.844828                   0\n",
       "19   0.283708  0.716292                   1\n",
       "20   0.283708  0.716292                   1\n",
       "21   0.283708  0.716292                   0\n",
       "22   0.283708  0.716292                   1\n",
       "23   0.480769  0.519231                   1\n",
       "24   0.301003  0.698997                   1\n",
       "25   0.301003  0.698997                   0\n",
       "26   0.283708  0.716292                   1\n",
       "27   0.301003  0.698997                   0\n",
       "28   0.301003  0.698997                   1\n",
       "29   0.301003  0.698997                   1\n",
       "30   0.287234  0.712766                   0\n",
       "31   0.301003  0.698997                   1\n",
       "32   0.155172  0.844828                   1\n",
       "33   0.301003  0.698997                   1\n",
       "34   0.376033  0.623967                   1\n",
       "35   0.376033  0.623967                   0\n",
       "36   0.376033  0.623967                   1\n",
       "37   0.301003  0.698997                   1\n",
       "38   0.500000  0.500000                   1\n",
       "39   0.376033  0.623967                   1\n",
       "40   0.283708  0.716292                   1\n",
       "41   0.283708  0.716292                   0\n",
       "42   0.301003  0.698997                   1\n",
       "43   0.155172  0.844828                   0\n",
       "44   0.480769  0.519231                   0\n",
       "45   0.480769  0.519231                   0\n",
       "46   0.500000  0.500000                   0\n",
       "47   0.115385  0.884615                   0\n",
       "48   0.376033  0.623967                   0\n",
       "49   0.155172  0.844828                   1\n",
       "50   0.287234  0.712766                   1\n",
       "51   0.155172  0.844828                   1\n",
       "52   0.301003  0.698997                   1\n",
       "53   0.155172  0.844828                   1\n",
       "54   0.376033  0.623967                   1\n",
       "55   0.155172  0.844828                   0\n",
       "56   0.376033  0.623967                   1\n",
       "57   0.283708  0.716292                   0\n",
       "58   0.480769  0.519231                   1\n",
       "59   0.283708  0.716292                   1\n",
       "60   0.287234  0.712766                   1\n",
       "61   0.376033  0.623967                   0\n",
       "62   0.115385  0.884615                   0\n",
       "63   0.283708  0.716292                   0\n",
       "64   0.376033  0.623967                   1\n",
       "65   0.376033  0.623967                   1\n",
       "66   0.283708  0.716292                   1\n",
       "67   0.115385  0.884615                   1\n",
       "68   0.155172  0.844828                   1\n",
       "69   0.287234  0.712766                   0\n",
       "70   0.480769  0.519231                   0\n",
       "71   0.283708  0.716292                   1\n",
       "72   0.301003  0.698997                   1\n",
       "73   0.480769  0.519231                   1\n",
       "74   0.155172  0.844828                   1\n",
       "75   0.283708  0.716292                   1\n",
       "76   0.155172  0.844828                   0\n",
       "77   0.376033  0.623967                   1\n",
       "78   0.287234  0.712766                   1\n",
       "79   0.301003  0.698997                   1\n",
       "80   0.155172  0.844828                   1\n",
       "81   0.283708  0.716292                   1\n",
       "82   0.283708  0.716292                   1\n",
       "83   0.283708  0.716292                   1\n",
       "84   0.480769  0.519231                   0\n",
       "85   0.480769  0.519231                   1\n",
       "86   0.301003  0.698997                   1\n",
       "87   0.287234  0.712766                   1\n",
       "88   0.480769  0.519231                   0\n",
       "89   0.301003  0.698997                   1\n",
       "90   0.376033  0.623967                   1\n",
       "91   0.500000  0.500000                   0\n",
       "92   0.283708  0.716292                   1\n",
       "93   0.301003  0.698997                   1\n",
       "94   0.301003  0.698997                   0\n",
       "95   0.287234  0.712766                   1\n",
       "96   0.480769  0.519231                   1\n",
       "97   0.301003  0.698997                   1\n",
       "98   0.480769  0.519231                   0\n",
       "99   0.155172  0.844828                   1\n",
       "100  0.287234  0.712766                   1\n",
       "101  0.283708  0.716292                   0\n",
       "102  0.283708  0.716292                   0\n",
       "103  0.155172  0.844828                   1\n",
       "104  0.287234  0.712766                   1\n",
       "105  0.287234  0.712766                   1\n",
       "106  0.376033  0.623967                   1\n",
       "107  0.480769  0.519231                   1\n",
       "108  0.301003  0.698997                   0\n",
       "109  0.480769  0.519231                   1\n",
       "110  0.301003  0.698997                   0\n",
       "111  0.301003  0.698997                   1\n",
       "112  0.480769  0.519231                   1\n",
       "113  0.155172  0.844828                   1\n",
       "114  0.376033  0.623967                   1\n",
       "115  0.283708  0.716292                   1\n",
       "116  0.301003  0.698997                   1\n",
       "117  0.301003  0.698997                   1\n",
       "118  0.155172  0.844828                   1\n",
       "119  0.480769  0.519231                   0\n",
       "120  0.301003  0.698997                   0\n",
       "121  0.287234  0.712766                   1\n",
       "122  0.283708  0.716292                   0\n",
       "123  0.283708  0.716292                   0\n",
       "124  0.376033  0.623967                   1\n",
       "125  0.287234  0.712766                   1\n",
       "126  0.376033  0.623967                   0\n",
       "127  0.283708  0.716292                   1\n",
       "128  0.301003  0.698997                   0\n",
       "129  0.480769  0.519231                   1\n",
       "130  0.283708  0.716292                   0\n",
       "131  0.301003  0.698997                   1\n",
       "132  0.376033  0.623967                   1\n",
       "133  0.287234  0.712766                   1\n",
       "134  0.283708  0.716292                   1\n",
       "135  0.301003  0.698997                   1\n",
       "136  0.283708  0.716292                   1\n",
       "137  0.301003  0.698997                   1\n",
       "138  0.283708  0.716292                   1\n",
       "139  0.376033  0.623967                   0\n",
       "140  0.480769  0.519231                   1\n",
       "141  0.376033  0.623967                   1\n",
       "142  0.376033  0.623967                   1\n",
       "143  0.287234  0.712766                   1\n",
       "144  0.376033  0.623967                   1\n",
       "145  0.283708  0.716292                   0\n",
       "146  0.283708  0.716292                   1\n",
       "147  0.155172  0.844828                   0\n",
       "148  0.283708  0.716292                   0\n",
       "149  0.480769  0.519231                   0\n",
       "150  0.283708  0.716292                   1\n",
       "151  0.301003  0.698997                   1\n",
       "152  0.301003  0.698997                   1\n",
       "153  0.376033  0.623967                   1\n",
       "154  0.301003  0.698997                   1\n",
       "155  0.301003  0.698997                   1\n",
       "156  0.480769  0.519231                   1\n",
       "157  0.283708  0.716292                   0\n",
       "158  0.283708  0.716292                   1\n",
       "159  0.301003  0.698997                   1\n",
       "160  0.480769  0.519231                   1\n",
       "161  0.283708  0.716292                   1\n",
       "162  0.480769  0.519231                   1\n",
       "163  0.301003  0.698997                   1\n",
       "164  0.155172  0.844828                   0\n",
       "165  0.287234  0.712766                   1\n",
       "166  0.376033  0.623967                   1\n",
       "167  0.376033  0.623967                   0\n",
       "168  0.376033  0.623967                   1\n",
       "169  0.301003  0.698997                   1\n",
       "170  0.480769  0.519231                   0\n",
       "171  0.283708  0.716292                   1\n",
       "172  0.480769  0.519231                   1\n",
       "173  0.283708  0.716292                   1\n",
       "174  0.376033  0.623967                   0\n",
       "175  0.287234  0.712766                   0\n",
       "176  0.480769  0.519231                   0\n",
       "177  0.287234  0.712766                   1\n",
       "178  0.287234  0.712766                   1\n",
       "179  0.287234  0.712766                   1\n",
       "180  0.301003  0.698997                   1\n",
       "181  0.287234  0.712766                   1\n",
       "182  0.283708  0.716292                   0\n",
       "183  0.480769  0.519231                   1\n",
       "184  0.283708  0.716292                   0\n",
       "185  0.376033  0.623967                   1\n",
       "186  0.376033  0.623967                   0\n",
       "187  0.376033  0.623967                   0\n",
       "188  0.283708  0.716292                   0\n",
       "189  0.283708  0.716292                   1\n",
       "190  0.283708  0.716292                   1\n",
       "191  0.283708  0.716292                   1\n",
       "192  0.283708  0.716292                   0\n",
       "193  0.480769  0.519231                   1\n",
       "194  0.376033  0.623967                   1\n",
       "195  0.283708  0.716292                   0\n",
       "196  0.283708  0.716292                   0\n",
       "197  0.480769  0.519231                   1\n",
       "198  0.301003  0.698997                   1\n",
       "199  0.155172  0.844828                   1\n",
       "200  0.301003  0.698997                   0\n",
       "201  0.376033  0.623967                   1\n",
       "202  0.283708  0.716292                   1\n",
       "203  0.500000  0.500000                   0\n",
       "204  0.155172  0.844828                   1\n",
       "205  0.155172  0.844828                   0\n",
       "206  0.283708  0.716292                   1\n",
       "207  0.376033  0.623967                   1\n",
       "208  0.287234  0.712766                   1\n",
       "209  0.376033  0.623967                   1\n",
       "210  0.301003  0.698997                   1\n",
       "211  0.301003  0.698997                   1\n",
       "212  0.155172  0.844828                   0\n",
       "213  0.301003  0.698997                   1\n",
       "214  0.376033  0.623967                   0\n",
       "215  0.283708  0.716292                   0\n",
       "216  0.376033  0.623967                   1\n",
       "217  0.480769  0.519231                   0\n",
       "218  0.283708  0.716292                   1\n",
       "219  0.480769  0.519231                   1\n",
       "220  0.283708  0.716292                   1\n",
       "221  0.301003  0.698997                   1\n",
       "222  0.376033  0.623967                   0\n",
       "223  0.500000  0.500000                   1\n",
       "224  0.283708  0.716292                   1\n",
       "225  0.480769  0.519231                   1\n",
       "226  0.283708  0.716292                   0\n",
       "227  0.283708  0.716292                   1\n",
       "228  0.301003  0.698997                   1\n",
       "229  0.301003  0.698997                   0\n",
       "230  0.500000  0.500000                   1\n",
       "231  0.376033  0.623967                   1\n",
       "232  0.376033  0.623967                   1\n",
       "233  0.283708  0.716292                   1\n",
       "234  0.376033  0.623967                   1\n",
       "235  0.155172  0.844828                   1\n",
       "236  0.283708  0.716292                   0\n",
       "237  0.283708  0.716292                   1\n",
       "238  0.287234  0.712766                   1\n",
       "239  0.480769  0.519231                   1\n",
       "240  0.480769  0.519231                   1\n",
       "241  0.283708  0.716292                   1\n",
       "242  0.376033  0.623967                   0\n",
       "243  0.480769  0.519231                   1\n",
       "244  0.283708  0.716292                   0\n",
       "245  0.376033  0.623967                   1\n",
       "246  0.283708  0.716292                   0\n",
       "247  0.283708  0.716292                   0\n",
       "248  0.283708  0.716292                   0\n",
       "249  0.155172  0.844828                   0\n",
       "250  0.155172  0.844828                   1\n",
       "251  0.500000  0.500000                   1\n",
       "252  0.301003  0.698997                   1\n",
       "253  0.283708  0.716292                   1\n",
       "254  0.155172  0.844828                   0\n",
       "255  0.500000  0.500000                   0\n",
       "256  0.301003  0.698997                   1\n",
       "257  0.283708  0.716292                   0\n",
       "258  0.301003  0.698997                   1\n",
       "259  0.283708  0.716292                   0\n",
       "260  0.500000  0.500000                   0\n",
       "261  0.155172  0.844828                   0\n",
       "262  0.283708  0.716292                   1\n",
       "263  0.115385  0.884615                   1\n",
       "264  0.301003  0.698997                   1\n",
       "265  0.155172  0.844828                   0\n",
       "266  0.301003  0.698997                   1\n",
       "267  0.283708  0.716292                   0\n",
       "268  0.301003  0.698997                   1\n",
       "269  0.301003  0.698997                   1\n",
       "270  0.500000  0.500000                   0\n",
       "271  0.480769  0.519231                   1\n",
       "272  0.283708  0.716292                   0\n",
       "273  0.115385  0.884615                   1\n",
       "274  0.155172  0.844828                   0\n",
       "275  0.155172  0.844828                   1\n",
       "276  0.283708  0.716292                   1\n",
       "277  0.155172  0.844828                   1\n",
       "278  0.283708  0.716292                   1\n",
       "279  0.155172  0.844828                   1\n",
       "280  0.115385  0.884615                   1\n",
       "281  0.480769  0.519231                   1\n",
       "282  0.301003  0.698997                   1\n",
       "283  0.301003  0.698997                   1\n",
       "284  0.155172  0.844828                   0\n",
       "285  0.301003  0.698997                   1\n",
       "286  0.283708  0.716292                   0\n",
       "287  0.376033  0.623967                   0\n",
       "288  0.480769  0.519231                   0\n",
       "289  0.283708  0.716292                   1\n",
       "290  0.376033  0.623967                   0\n",
       "291  0.301003  0.698997                   1\n",
       "292  0.301003  0.698997                   1\n",
       "293  0.283708  0.716292                   0\n",
       "294  0.283708  0.716292                   1\n",
       "295  0.283708  0.716292                   1\n",
       "296  0.376033  0.623967                   1\n",
       "297  0.480769  0.519231                   0\n",
       "298  0.155172  0.844828                   1\n",
       "299  0.301003  0.698997                   1\n",
       "300  0.376033  0.623967                   1\n",
       "301  0.283708  0.716292                   0\n",
       "302  0.480769  0.519231                   1\n",
       "303  0.376033  0.623967                   1\n",
       "304  0.376033  0.623967                   0\n",
       "305  0.301003  0.698997                   1\n",
       "306  0.376033  0.623967                   0\n",
       "307  0.376033  0.623967                   1\n",
       "308  0.283708  0.716292                   1\n",
       "309  0.376033  0.623967                   0\n",
       "310  0.301003  0.698997                   1\n",
       "311  0.480769  0.519231                   0\n",
       "312  0.283708  0.716292                   1\n",
       "313  0.376033  0.623967                   1\n",
       "314  0.155172  0.844828                   1\n",
       "315  0.301003  0.698997                   1\n",
       "316  0.376033  0.623967                   1\n",
       "317  0.480769  0.519231                   0\n",
       "318  0.376033  0.623967                   1\n",
       "319  0.500000  0.500000                   0\n",
       "320  0.301003  0.698997                   1\n",
       "321  0.480769  0.519231                   0\n",
       "322  0.155172  0.844828                   1\n",
       "323  0.283708  0.716292                   1\n",
       "324  0.301003  0.698997                   1\n",
       "325  0.155172  0.844828                   1\n",
       "326  0.480769  0.519231                   1\n",
       "327  0.301003  0.698997                   1\n",
       "328  0.283708  0.716292                   1\n",
       "329  0.283708  0.716292                   0\n",
       "330  0.301003  0.698997                   1\n",
       "331  0.283708  0.716292                   0\n",
       "332  0.155172  0.844828                   1\n",
       "333  0.480769  0.519231                   0\n",
       "334  0.376033  0.623967                   0\n",
       "335  0.283708  0.716292                   1\n",
       "336  0.376033  0.623967                   1\n",
       "337  0.287234  0.712766                   1\n",
       "338  0.480769  0.519231                   0\n",
       "339  0.376033  0.623967                   1\n",
       "340  0.283708  0.716292                   1\n",
       "341  0.480769  0.519231                   1\n",
       "342  0.376033  0.623967                   1\n",
       "343  0.283708  0.716292                   1\n",
       "344  0.301003  0.698997                   1\n",
       "345  0.283708  0.716292                   1\n",
       "346  0.155172  0.844828                   0\n",
       "347  0.376033  0.623967                   0\n",
       "348  0.301003  0.698997                   1\n",
       "349  0.301003  0.698997                   1\n",
       "350  0.283708  0.716292                   1\n",
       "351  0.155172  0.844828                   1\n",
       "352  0.287234  0.712766                   1\n",
       "353  0.155172  0.844828                   1\n",
       "354  0.283708  0.716292                   0\n",
       "355  0.376033  0.623967                   1\n",
       "356  0.301003  0.698997                   0\n",
       "357  0.301003  0.698997                   1\n",
       "358  0.376033  0.623967                   0\n",
       "359  0.155172  0.844828                   0\n",
       "360  0.155172  0.844828                   1\n",
       "361  0.283708  0.716292                   1\n",
       "362  0.301003  0.698997                   0\n",
       "363  0.155172  0.844828                   1\n",
       "364  0.155172  0.844828                   1\n",
       "365  0.376033  0.623967                   0\n",
       "366  0.480769  0.519231                   0\n",
       "367  0.376033  0.623967                   1\n",
       "368  0.155172  0.844828                   0\n",
       "369  0.301003  0.698997                   1\n",
       "370  0.480769  0.519231                   0\n",
       "371  0.376033  0.623967                   1\n",
       "372  0.155172  0.844828                   1\n",
       "373  0.376033  0.623967                   0\n",
       "374  0.480769  0.519231                   1\n",
       "375  0.301003  0.698997                   1\n",
       "376  0.480769  0.519231                   1\n",
       "377  0.283708  0.716292                   0\n",
       "378  0.301003  0.698997                   1\n",
       "379  0.283708  0.716292                   0\n",
       "380  0.480769  0.519231                   1\n",
       "381  0.376033  0.623967                   0\n",
       "382  0.283708  0.716292                   1\n",
       "383  0.301003  0.698997                   0\n",
       "384  0.287234  0.712766                   1\n",
       "385  0.301003  0.698997                   1\n",
       "386  0.301003  0.698997                   1\n",
       "387  0.283708  0.716292                   0\n",
       "388  0.500000  0.500000                   1\n",
       "389  0.500000  0.500000                   0\n",
       "390  0.287234  0.712766                   1\n",
       "391  0.376033  0.623967                   1\n",
       "392  0.301003  0.698997                   1\n",
       "393  0.115385  0.884615                   1\n",
       "394  0.155172  0.844828                   1\n",
       "395  0.480769  0.519231                   1\n",
       "396  0.283708  0.716292                   0\n",
       "397  0.155172  0.844828                   1\n",
       "398  0.480769  0.519231                   1\n",
       "399  0.283708  0.716292                   0\n",
       "400  0.301003  0.698997                   1\n",
       "401  0.283708  0.716292                   0\n",
       "402  0.301003  0.698997                   0\n",
       "403  0.376033  0.623967                   0\n",
       "404  0.480769  0.519231                   1\n",
       "405  0.283708  0.716292                   1\n",
       "406  0.155172  0.844828                   0\n",
       "407  0.283708  0.716292                   1\n",
       "408  0.155172  0.844828                   1\n",
       "409  0.283708  0.716292                   0\n",
       "410  0.115385  0.884615                   1\n",
       "411  0.283708  0.716292                   1\n",
       "412  0.287234  0.712766                   1\n",
       "413  0.283708  0.716292                   1\n",
       "414  0.500000  0.500000                   1\n",
       "415  0.287234  0.712766                   1\n",
       "416  0.283708  0.716292                   1\n",
       "417  0.480769  0.519231                   0\n",
       "418  0.283708  0.716292                   1\n",
       "419  0.155172  0.844828                   0\n",
       "420  0.480769  0.519231                   0\n",
       "421  0.301003  0.698997                   1\n",
       "422  0.283708  0.716292                   1\n",
       "423  0.301003  0.698997                   1\n",
       "424  0.283708  0.716292                   1\n",
       "425  0.301003  0.698997                   1\n",
       "426  0.376033  0.623967                   0\n",
       "427  0.301003  0.698997                   1\n",
       "428  0.376033  0.623967                   0\n",
       "429  0.155172  0.844828                   1\n",
       "430  0.283708  0.716292                   0\n",
       "431  0.376033  0.623967                   0\n",
       "432  0.301003  0.698997                   1\n",
       "433  0.480769  0.519231                   0\n",
       "434  0.287234  0.712766                   1\n",
       "435  0.376033  0.623967                   0\n",
       "436  0.376033  0.623967                   1\n",
       "437  0.283708  0.716292                   1\n",
       "438  0.301003  0.698997                   0\n",
       "439  0.301003  0.698997                   1\n",
       "440  0.283708  0.716292                   0\n",
       "441  0.287234  0.712766                   0\n",
       "442  0.480769  0.519231                   1\n",
       "443  0.480769  0.519231                   1\n",
       "444  0.301003  0.698997                   1\n",
       "445  0.155172  0.844828                   1\n",
       "446  0.376033  0.623967                   1\n",
       "447  0.376033  0.623967                   1\n",
       "448  0.301003  0.698997                   1\n",
       "449  0.287234  0.712766                   1\n",
       "450  0.480769  0.519231                   0\n",
       "451  0.376033  0.623967                   0\n",
       "452  0.283708  0.716292                   1\n",
       "453  0.376033  0.623967                   1\n",
       "454  0.301003  0.698997                   1\n",
       "455  0.115385  0.884615                   1\n",
       "456  0.376033  0.623967                   1\n",
       "457  0.301003  0.698997                   1\n",
       "458  0.283708  0.716292                   1\n",
       "459  0.155172  0.844828                   1\n",
       "460  0.155172  0.844828                   0\n",
       "461  0.283708  0.716292                   1\n",
       "462  0.283708  0.716292                   0\n",
       "463  0.283708  0.716292                   0\n",
       "464  0.480769  0.519231                   0\n",
       "465  0.480769  0.519231                   1\n",
       "466  0.301003  0.698997                   0\n",
       "467  0.480769  0.519231                   1\n",
       "468  0.155172  0.844828                   1\n",
       "469  0.376033  0.623967                   0\n",
       "470  0.283708  0.716292                   0\n",
       "471  0.283708  0.716292                   0\n",
       "472  0.283708  0.716292                   1\n",
       "473  0.155172  0.844828                   1\n",
       "474  0.283708  0.716292                   1\n",
       "475  0.283708  0.716292                   1\n",
       "476  0.155172  0.844828                   1\n",
       "477  0.500000  0.500000                   1\n",
       "478  0.376033  0.623967                   1\n",
       "479  0.283708  0.716292                   1\n",
       "480  0.376033  0.623967                   1\n",
       "481  0.283708  0.716292                   1\n",
       "482  0.283708  0.716292                   1\n",
       "483  0.376033  0.623967                   1\n",
       "484  0.301003  0.698997                   1\n",
       "485  0.287234  0.712766                   0\n",
       "486  0.301003  0.698997                   1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# joining predictions with y_test\n",
    "predictions_df.join(pd.Series(y_test.reset_index(drop = True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1ccaffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5365"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the prediction threshold\n",
    "\n",
    "# placeholder column\n",
    "predictions_df['predictions_at_60'] = 0\n",
    "\n",
    "\n",
    "# looping to flag at 0.60 for positive class\n",
    "for index, column in predictions_df.iterrows():\n",
    "    \n",
    "    if  predictions_df.loc[ index, 'positive' ] >= 0.60:\n",
    "        predictions_df.loc[ index , 'predictions_at_60'] = 1\n",
    "        \n",
    "\n",
    "# outputting AUC score (p = 0.06 for positive class)\n",
    "auc_score = roc_auc_score(y_true  = y_test,\n",
    "                        y_score = predictions_df['predictions_at_60']).round(4) # auc\n",
    "\n",
    "# calling auc score\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9056e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test score of a logistic regression model on the training set, and round the result to 4 decimal places\n",
    "final_test_gap = logreg_fit.score(x_train, y_train).round(4)\n",
    "# the training score of a logistic regression model on the test set\n",
    "final_train_gap = logreg_fit.score(x_test, y_test).round(4)\n",
    "# absolute difference between the test and training scores, round the result to 4 decimal places\n",
    "final_gap = abs((logreg_fit.score(x_train, y_train).round(4)) - (logreg_fit.score(x_test, y_test).round(4))).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "237621f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final_Model_Name:     DecisionTreeClassifier\n",
      "Train_Test_Gap: 0.0004\n",
      "AUC_Score: 0.5365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dynamically printing results\n",
    "Logistic_Regression =  f\"\"\"\\\n",
    "Final_Model_Name:     {final_model_name}\n",
    "Train_Test_Gap: {final_gap}\n",
    "AUC_Score: {auc_score}\n",
    "\"\"\"\n",
    "\n",
    "# print the Logistic Regression string variable\n",
    "print(Logistic_Regression) # printing the designed model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
